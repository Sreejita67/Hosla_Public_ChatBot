import warnings
import sys
if hasattr(sys.stdout, "reconfigure"):
    sys.stdout.reconfigure(encoding='utf-8')
import os
import time 
import pathlib
import unicodedata
import re, requests
import pandas as pd
import pytz
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer, util
from datetime import datetime, timedelta
import urllib3
from googletrans import Translator
from langdetect import detect
from difflib import SequenceMatcher
from functools import lru_cache
from difflib import get_close_matches
import xml.etree.ElementTree as ET
from huggingface_hub import HfApi, HfFolder, Repository
from io import StringIO

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ================================================
# üìÑ Load FAQ CSV from Google Sheet or fallback
# ================================================
GOOGLE_SHEET_CSV_URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vT8IArJoxgQ2EL2fQJn_rUVozWqJbz-n0Qn42rTMDHHZezCbn5MEa-0TcvRfPiEGPyDj3W96LkRFwSH/pub?gid=0&single=true&output=csv"

def load_faq_from_google_sheet():
    try:
        response = requests.get(GOOGLE_SHEET_CSV_URL, timeout=10)
        if response.status_code == 200:
            csv_data = StringIO(response.text)
            df = pd.read_csv(csv_data, usecols=[0, 1], names=["Question", "Answer"], header=0, encoding='utf-8', on_bad_lines='skip')
            print("‚úÖ Loaded FAQ from live Google Sheet.")
            return df.dropna(subset=['Question', 'Answer'])
        else:
            print(f"‚ö†Ô∏è Failed to fetch from Google Sheet. Status: {response.status_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è Exception while loading from Google Sheet: {e}")
    
    return None

# Load from Google Sheet, or fallback to local file
faq_df = load_faq_from_google_sheet()
if faq_df is None:
    FAQ_FILE = "FAQ - Sheet1.csv"
    if not os.path.exists(FAQ_FILE):
        raise FileNotFoundError(f"‚ùå File '{FAQ_FILE}' not found.")
    faq_df = pd.read_csv(FAQ_FILE, usecols=[0, 1], names=["Question", "Answer"], header=0, encoding='utf-8', on_bad_lines='skip')
    faq_df = faq_df.dropna(subset=['Question', 'Answer'])
    print("‚úÖ Loaded FAQ from local CSV.")

df = faq_df 

# Load BERT model and encode questions
model = SentenceTransformer('all-MiniLM-L6-v2')
question_embeddings = model.encode(faq_df['Question'].tolist(), convert_to_tensor=True)

translator = Translator()

# Supported languages
SUPPORTED_LANGUAGES = ['en', 'bn', 'hi', 'ta', 'te', 'mr', 'pa', 'or']

upcoming_event_keywords = [
    "upcoming event", "upcoming events", "future events", "hosla events", "upcoming programs",
    "sessions happening", "event list", "‡¶Ü‡¶∏‡¶®‡ßç‡¶® ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü", "‡¶Ü‡¶∏‡¶®‡ßç‡¶® ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®", "events",
    "‡§Ü‡§ó‡§æ‡§Æ‡•Ä ‡§á‡§µ‡•á‡§Ç‡§ü", "‡§Ü‡§ó‡§æ‡§Æ‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ", "‡§Ü‡§ó‡§æ‡§Æ‡•Ä ‡§ò‡§ü‡§®‡§æ"
]

past_event_keywords = [
    "past events", "previous sessions", "earlier events", "events already held", "past programs",
    "‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü", "‡¶ó‡¶§ ‡¶Ö‡¶®‡ßÅ‡¶∑‡ßç‡¶†‡¶æ‡¶®", "‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ò‡¶ü‡¶®‡¶æ", 
    "‡§™‡§ø‡§õ‡§≤‡•á ‡§á‡§µ‡•á‡§Ç‡§ü", "‡§™‡•Ç‡§∞‡•ç‡§µ ‡§ï‡•Ä ‡§ò‡§ü‡§®‡§æ‡§è‡§Å", "‡§™‡§ø‡§õ‡§≤‡•á ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ"
]

def build_medline_topic_index_api():
    try:
        url = "https://wsearch.nlm.nih.gov/ws/query?db=healthTopics&term=health&retmax=1000"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)

        soup = BeautifulSoup(res.content, "lxml")

        topics = {}
        for doc in soup.find_all("document"):
            url = doc.get("url")
            title_tag = doc.find("content", attrs={"name": "title"})
            if title_tag and url:
                clean_title = BeautifulSoup(title_tag.text, "html.parser").get_text()
                topics[clean_title.strip().lower()] = url
        return topics
    except Exception as e:
        print(f"‚ùå Failed to load MedlinePlus topics via API: {e}")
        return {}

medical_cache = {}
medline_topics = build_medline_topic_index_api()   

def is_health_query(user_input):
    # Check if the cleaned input loosely matches any MedlinePlus topic
    user_input_clean = user_input.strip().lower()
    matches = get_close_matches(user_input_clean, medline_topics.keys(), n=1, cutoff=0.6)
    return bool(matches)
 
def fetch_medline_info(user_query, user_lang="en"):
    query_key = user_query.strip().lower()

    # Check cached response
    if query_key in medical_cache:
        cached = medical_cache[query_key]
        return translator.translate(cached, dest=user_lang).text if user_lang != "en" else cached

    # Match query to Medline topics
    best_match = get_close_matches(query_key, medline_topics.keys(), n=1, cutoff=0.6)
    if not best_match:
        return None

    topic_title = best_match[0]
    topic_url = medline_topics[topic_title]

    # Scrape the topic page
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(topic_url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        # Try to extract meaningful paragraph
        content_block = soup.select_one(".main-content p") or soup.select_one("p")
        if not content_block:
            return None

        content = content_block.get_text(strip=True)
        full_text = f"{content}\n\nüîó More info: {topic_url}"

        # Cache it
        medical_cache[query_key] = full_text

        return translator.translate(full_text, dest=user_lang).text if user_lang != "en" else full_text

    except Exception as e:
        print(f"‚ùå Error fetching Medline topic: {e}")
        return None

def detect_language_safe(text: str) -> str:
    try:
        # If only ASCII characters, assume English
        if all(ord(c) < 128 for c in text):
            return "en"

        # Use langdetect
        lang = detect(text)

        # If not in supported list, try to infer from Unicode ranges
        if lang not in SUPPORTED_LANGUAGES:
            if re.search(r'[‡¶Ä-‡ßø]', text): return "bn"   # Bengali
            if re.search(r'[‡§Ä-‡•ø]', text): return "hi"   # Hindi
            if re.search(r'[‡ÆÄ-‡Øø]', text): return "ta"   # Tamil
            if re.search(r'[‡∞Ä-‡±ø]', text): return "te"   # Telugu
            if re.search(r'[‡®Ä-‡©ø]', text): return "pa"   # Punjabi
            if re.search(r'[‡¨Ä-‡≠ø]', text): return "or"   # Odia
            if re.search(r'[‡§Æ‡§∞‡§æ‡§†‡•Ä]', text): return "mr" # Marathi (fallback if needed)
            return "en"

        return lang
    except:
        return "en"

def clean_query(text):
    return re.sub(r'[^\w\s]', '', text).strip().lower()

def similar(a, b):
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def show_event_feedback(user_lang="en"):
    try:
        csv_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vT8IArJoxgQ2EL2fQJn_rUVozWqJbz-n0Qn42rTMDHHZezCbn5MEa-0TcvRfPiEGPyDj3W96LkRFwSH/pub?gid=1271142221&single=true&output=csv"

        response = requests.get(csv_url, timeout=5)  
        response.raise_for_status()  # Raise error for bad response

        df_event = pd.read_csv(StringIO(response.text))

        if df_event.empty:
            text = "üôÅ No members' feedback available right now."
            return translator.translate(text, dest=user_lang).text if user_lang != "en" else text

        responses = []
        for _, row in df_event.iterrows():
            name = row.get("Name", "Someone")
            relation = row.get("Title (Relation to Hosla)", "")
            message = str(row.get("Description (Message)", "")).strip()
            if not message:
                continue

            try:
                said_translated = translator.translate("said", dest=user_lang).text if user_lang != "en" else "said"
            except:
                said_translated = "said"

            base_line = f"{name} ({relation}) {said_translated}: \"{message}\""

            if user_lang != "en":
                try:
                    translated = translator.translate(message, dest=user_lang).text
                    base_line += f"\nüó£Ô∏è Translated: \"{translated}\""
                except:
                    pass

            responses.append(base_line)

        return "\n\n".join(responses[:3])

    except Exception as e:
        print(f"‚ùå Error reading public sheet: {e}")
        text = "üôÅ Sorry, couldn't load event feedback right now."
        return translator.translate(text, dest=user_lang).text if user_lang != "en" else text



def translate_text(text, dest_language="en"):
    try:
        translated = translator.translate(text, dest=dest_language)
        return translated.text
    except Exception:
        return text  # Fallback to original if translation fails

def show_events_info(user_input):
    lang = detect_language_safe(user_input.lower())

    # Detect event type and choose correct sheet URL
    if any(kw in user_input.lower() for kw in upcoming_event_keywords):
        sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vT8IArJoxgQ2EL2fQJn_rUVozWqJbz-n0Qn42rTMDHHZezCbn5MEa-0TcvRfPiEGPyDj3W96LkRFwSH/pub?gid=1925531775&single=true&output=csv"
    elif any(kw in user_input.lower() for kw in past_event_keywords):
        sheet_url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vT8IArJoxgQ2EL2fQJn_rUVozWqJbz-n0Qn42rTMDHHZezCbn5MEa-0TcvRfPiEGPyDj3W96LkRFwSH/pub?gid=1834108950&single=true&output=csv"
    else:
        return translate_text("‚ùì Please specify whether you want to see past or upcoming events.", lang)

    # Read CSV data
    try:
        df = pd.read_csv(sheet_url)
    except Exception as e:
        return translate_text(f"‚ö†Ô∏è Could not load event data: {e}", lang)

    # Normalize columns
    df.columns = [col.strip().lower() for col in df.columns]
    required_cols = ["event", "event description", "date", "time", "location"]

    if not all(col in df.columns for col in required_cols):
        return translate_text("‚ö†Ô∏è The events sheet is missing required columns.", lang)

    # Parse datetime (used only for display formatting)
    try:
        df["datetime"] = pd.to_datetime(df["date"] + " " + df["time"], dayfirst=True, errors='coerce')
    except:
        df["datetime"] = None

    # Prepare responses
    responses = []
    for _, row in df.iterrows():
        event = translate_text(str(row.get("event", "Unnamed Event")), lang)
        desc = translate_text(str(row.get("event description", "")), lang)
        location = translate_text(str(row.get("location", "Unknown")), lang)

        if pd.notnull(row.get("datetime")):
            datetime_str = row["datetime"].strftime("%B %d, %Y at %I:%M %p")
        else:
            datetime_str = f"{row.get('date', '')} {row.get('time', '')}"

        response = f"üìå {event}\nüìù {desc}\nüìç {location}\nüóìÔ∏è {datetime_str}"
        responses.append(response)

    if not responses:
        return translate_text("No events found in the sheet.", lang)

    return "\n\n".join(responses) + f"\n\n{translate_text('These are the events we found.', lang)}"
    
def normalize_text(text):
    return re.sub(r'[^\w\s]', '', text.lower().strip())

def get_answer_from_faq(query: str, is_guest: bool = True) -> tuple:
    user_input = query

        # Emergency fuzzy check (multilingual + typo-tolerant)
    from difflib import get_close_matches

    def contains_emergency_keyword(text):
        keywords = ["emergency", "‡¶ú‡¶∞‡ßÅ‡¶∞‡ßÄ", "‡§Ü‡§™‡§æ‡§§‡§ï‡§æ‡§≤","‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßá‡¶æ‡¶ú‡¶®","help"]
        text_words = text.lower().split()
        for word in text_words:
            if get_close_matches(word, keywords, n=1, cutoff=0.8):
                return True
        return False

    if contains_emergency_keyword(user_input):
        emergency_msg = (
            "üö® I understand you need an urgent help. Please stay calm.\n"
            "For any help during any emergency,Please dial +91 78110 09309.\n"
            "Hosla is always here for you. ‚ù§Ô∏è"
        )
        try:
            user_lang = detect_language_safe(user_input)
            if user_lang != "en":
                emergency_msg = translator.translate(emergency_msg, dest=user_lang).text
        except:
            pass
        return emergency_msg, False

    user_input_lower = normalize_text(user_input)
    user_is_guest = is_guest
    
    # üåê Multilingual restricted keywords
    restricted_keywords = [
        "mental age", "happiness quotient", "mental health",
        "‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø", "‡§Æ‡§®‡•ã‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø", "‡§ñ‡§º‡•Å‡§∂‡•Ä ‡§ï‡§æ ‡§∏‡•ç‡§§‡§∞", "‡§Ü‡§®‡§Ç‡§¶ ‡§∏‡•ç‡§§‡§∞",
        "‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø", "‡§Ü‡§®‡§Ç‡§¶ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£",
        "‡ÆÆ‡Æ©‡Æ®‡Æ≤‡ÆÆ‡Øç", "‡ÆÆ‡Æ©‡Æö‡Øç ‡Æö‡ØÅ‡Æï‡Ææ‡Æ§‡Ææ‡Æ∞‡ÆÆ‡Øç", "‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡ÆÖ‡Æ≥‡Æµ‡ØÄ‡Æü‡ØÅ",
        "‡®Æ‡®æ‡®®‡®∏‡®ø‡®ï ‡®∏‡®ø‡®π‡®§", "‡®ñ‡©Å‡®∏‡®º‡©Ä ‡®¶‡®æ ‡®™‡©±‡®ß‡®∞",
        "‡∞Æ‡∞æ‡∞®‡∞∏‡∞ø‡∞ï ‡∞Ü‡∞∞‡±ã‡∞ó‡±ç‡∞Ø‡∞Ç", "‡∞Ü‡∞®‡∞Ç‡∞¶ ‡∞∏‡±ç‡∞•‡∞æ‡∞Ø‡∞ø",
        "‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø", "‡¶Ü‡¶®‡¶®‡ßç‡¶¶‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ", "‡¶Æ‡¶æ‡¶®‡¶∏‡¶ø‡¶ï ‡¶¨‡¶Ø‡¶º‡¶∏"
    ]

    try:
        user_lang = detect_language_safe(user_input)
    except:
        user_lang = "en"

    # üîí Guest restriction check
    if user_is_guest:
        for keyword in restricted_keywords:
            if normalize_text(keyword) in user_input_lower:
                response_text = "üö´ Sorry, this feature is for members only."
                try:
                    if user_lang != "en":
                        response_text = translator.translate(response_text, dest=user_lang).text
                except:
                    pass
                return response_text, False

    # üåê Translation for semantic matching
    try:
        translated_input = translator.translate(user_input, dest="en").text if user_lang != "en" else user_input
    except:
        translated_input = user_input

    translated_input_lower = normalize_text(translated_input)
    original_input_lower = user_input.lower().strip()
    # üëã Greeting response
    greeting_inputs = [
        "hi", "hello", "hey", "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã", "‡¶®‡¶Æ‡§∏‡•ç‡§§‡ßá", "‡¶π‡¶æ‡¶á", "‡¶π‡ßá", "‡¶π‡ßá‡¶≤‡ßã", "‡¶∏‡ßÅ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶§", "‡¶∂‡ßÅ‡¶≠ ‡¶∏‡¶ï‡¶æ‡¶≤",
        "good morning", "good evening", "good afternoon"
    ]
    normalized_input = original_input_lower.replace(",", "").replace("?", "")
    if normalized_input in [g.lower() for g in greeting_inputs]:
        greeting_response = "Hi there! üëã I'm Hosla Public Chatbot. How can I assist you today?"
        try:
            if user_lang != "en":
                greeting_response = translator.translate(greeting_response, dest=user_lang).text
        except:
            pass
        return greeting_response, False

    # üåê Multilingual Feedback Trigger
    feedback_triggers_multilingual = {
        "en": ["user feedback", "what members say", "hosla feedback", "event feedback", "audience feedback", "feedback"],
        "hi": ["‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ", "‡§∏‡§¶‡§∏‡•ç‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§π‡§§‡•á ‡§π‡•à‡§Ç", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ", "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ", "‡§¶‡§∞‡•ç‡§∂‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ"],
        "bn": ["‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ", "‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø‡¶∞‡¶æ ‡¶ï‡ßÄ ‡¶¨‡¶≤‡ßá‡¶®", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ", "‡¶á‡¶≠‡ßá‡¶®‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ", "‡¶¶‡¶∞‡ßç‡¶∂‡¶ï‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ"],
        "pa": ["‡®™‡©ç‡®∞‡®§‡©Ä‡®ï‡®ø‡®∞‡®ø‡®Ü", "‡®Æ‡©à‡®Ç‡®¨‡®∞ ‡®ï‡©Ä ‡®ï‡®π‡®ø‡©∞‡®¶‡©á ‡®π‡®®", "‡®π‡©å‡®∏‡®≤‡®æ ‡®´‡©Ä‡®°‡®¨‡©à‡®ï", "‡®à‡®µ‡©à‡®Ç‡®ü ‡®´‡©Ä‡®°‡®¨‡©à‡®ï", "‡®¶‡®∞‡®∏‡®º‡®ï‡®æ‡®Ç ‡®¶‡©Ä ‡®™‡©ç‡®∞‡®§‡©Ä‡®ï‡®ø‡®∞‡®ø‡®Ü"],
        "ta": ["‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ", "‡Æâ‡Æ±‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æ©‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æö‡Øä‡Æ≤‡Øç‡Æµ‡Æ§‡ØÅ", "‡Æπ‡Øä‡Æ∏‡Øç‡Æ≤‡Ææ ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ", "‡Æ®‡Æø‡Æï‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ", "‡Æ™‡Ææ‡Æ∞‡Øç‡Æµ‡Øà‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ"],
        "te": ["‡∞Ö‡∞≠‡∞ø‡∞™‡±ç‡∞∞‡∞æ‡∞Ø‡∞Ç", "‡∞∏‡∞≠‡±ç‡∞Ø‡±Å‡∞≤‡±Å ‡∞è‡∞Æ‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å", "‡∞π‡±ã‡∞∏‡±ç‡∞≤‡∞æ ‡∞Ö‡∞≠‡∞ø‡∞™‡±ç‡∞∞‡∞æ‡∞Ø‡∞Ç", "‡∞à‡∞µ‡±Ü‡∞Ç‡∞ü‡±ç ‡∞Ö‡∞≠‡∞ø‡∞™‡±ç‡∞∞‡∞æ‡∞Ø‡∞Ç", "‡∞™‡±ç‡∞∞‡±á‡∞ï‡±ç‡∞∑‡∞ï‡±Å‡∞≤ ‡∞Ö‡∞≠‡∞ø‡∞™‡±ç‡∞∞‡∞æ‡∞Ø‡∞Ç"],
        "mr": ["‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ", "‡§∏‡§¶‡§∏‡•ç‡§Ø ‡§ï‡§æ‡§Ø ‡§Æ‡•ç‡§π‡§£‡§§‡§æ‡§§", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡§æ‡§Ø", "‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡§æ‡§Ø", "‡§™‡•ç‡§∞‡•á‡§ï‡•ç‡§∑‡§ï ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡§æ‡§Ø"],
        "or": ["‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï‡≠ç‡¨∞‡¨ø‡≠ü‡¨æ", "‡¨∏‡¨¶‡¨∏‡≠ç‡≠ü‡¨Æ‡¨æ‡¨®‡≠á ‡¨ï‡¨£ ‡¨ï‡¨π‡¨®‡≠ç‡¨§‡¨ø", "‡¨π‡≠ã‡¨∏‡¨≤‡¨æ ‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï‡≠ç‡¨∞‡¨ø‡≠ü‡¨æ", "‡¨á‡¨≠‡≠á‡¨£‡≠ç‡¨ü ‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï‡≠ç‡¨∞‡¨ø‡≠ü‡¨æ", "‡¨¶‡¨∞‡≠ç‡¨∂‡¨ï ‡¨™‡≠ç‡¨∞‡¨§‡¨ø‡¨ï‡≠ç‡¨∞‡¨ø‡≠ü‡¨æ"]
    }

    def normalize(text):
        return unicodedata.normalize("NFKC", text).strip().lower()
    
    def similar(a, b):
        return SequenceMatcher(None, a, b).ratio()
        
    def check_feedback_trigger(user_input, user_lang):
        triggers = feedback_triggers_multilingual.get(user_lang, feedback_triggers_multilingual["en"])
        user_input = normalize(user_input)
        for trigger in triggers:
            if similar(user_input, normalize(trigger)) > 0.85:
               return True
        return False

    user_input_lower = user_input.lower()

    if check_feedback_trigger(user_input_lower, user_lang):
       return show_event_feedback(user_lang), False


    if any(kw in query.lower() for kw in upcoming_event_keywords + past_event_keywords):
       return show_events_info(query), False

        
    # ‚ú® Multilingual internship keywords
    internship_keywords_by_lang = {
        "en": ["intern", "internship", "volunteer", "work with hosla", "join hosla", "career", "part of hosla", "can I work as an intern", "want to be a part of hosla"],
        "hi": ["‡§á‡§Ç‡§ü‡§∞‡•ç‡§®", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§Æ", "‡§∏‡•ç‡§µ‡§Ø‡§Ç‡§∏‡•á‡§µ‡§ï", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§Æ‡•á‡§Ç ‡§ú‡•Å‡§°‡§º‡•á‡§Ç"],
        "bn": ["‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶®", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ‡¶§‡ßá ‡¶ï‡¶æ‡¶ú", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ ‡¶§‡ßá ‡¶ï‡¶æ‡¶ú", "‡¶∏‡ßç‡¶¨‡ßá‡¶ö‡ßç‡¶õ‡¶æ‡¶∏‡ßá‡¶¨‡¶ï", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ ‡¶§‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶π‡¶á","‡¶π‡ßã‡¶∏‡¶≤‡¶æ‡¶∞ ‡¶Ö‡¶Ç‡¶∂ ‡¶π‡¶§‡ßá ‡¶ö‡¶æ‡¶á", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ ‡¶§‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶π‡¶§‡ßá ‡¶ö‡¶æ‡¶á"],
        "ta": ["‡Æá‡Æ£‡Øç‡Æü‡Æ∞‡Øç‡Æ©‡Øç‡Æ∑‡Æø‡Æ™‡Øç", "‡Æπ‡Øã‡Æ∏‡Øç‡Æ≤‡Ææ‡Æµ‡Æø‡Æ≤‡Øç ‡Æµ‡Øá‡Æ≤‡Øà", "‡Æµ‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™ ‡Æö‡Øá‡Æµ‡Øà‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç"],
        "te": ["‡∞á‡∞Ç‡∞ü‡∞∞‡±ç‡∞®‡±ç", "‡∞á‡∞Ç‡∞ü‡∞∞‡±ç‡∞®‡±ç‚Äå‡∞∑‡∞ø‡∞™‡±ç", "‡∞µ‡∞æ‡∞≤‡∞Ç‡∞ü‡±Ä‡∞∞‡±ç", "‡∞π‡±ã‡∞∏‡±ç‡∞≤‡∞æ‡∞≤‡±ã ‡∞™‡∞®‡∞ø ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞ø", "‡∞π‡±ã‡∞∏‡±ç‡∞≤‡∞æ‡∞≤‡±ã ‡∞ö‡±á‡∞∞‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å", "‡∞â‡∞¶‡±ç‡∞Ø‡±ã‡∞ó ‡∞Ö‡∞µ‡∞ï‡∞æ‡∞∂‡∞æ‡∞≤‡±Å", "‡∞π‡±ã‡∞∏‡±ç‡∞≤‡∞æ ‡∞≠‡∞æ‡∞ó‡∞Ç ‡∞ï‡∞æ‡∞µ‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å"],
        "pa": ["‡®á‡©∞‡®ü‡®∞‡®®", "‡®á‡©∞‡®ü‡®∞‡®®‡®∏‡®º‡®ø‡®™", "‡®∏‡©á‡®µ‡®æ ‡®ï‡®∞‡®§‡®æ", "‡®π‡©å‡®∏‡®≤‡®æ ‡®µ‡®ø‡©±‡®ö ‡®ï‡©∞‡®Æ ‡®ï‡®∞‡®®‡®æ ‡®ö‡®æ‡®π‡©Å‡©∞‡®¶‡®æ ‡®π‡®æ‡®Ç","‡®π‡©å‡®∏‡®≤‡®æ ‡®µ‡®ø‡©±‡®ö ‡®∂‡®æ‡®Æ‡®ø‡®≤ ‡®π‡©ã‡®£‡®æ", "‡®ï‡©à‡®∞‡©Ä‡®Ö‡®∞ ‡®Æ‡©å‡®ï‡©á", "‡®π‡©å‡®∏‡®≤‡®æ ‡®¶‡®æ ‡®π‡®ø‡©±‡®∏‡®æ ‡®¨‡®£‡®®‡®æ"],
        "mr": ["‡§á‡§Ç‡§ü‡§∞‡•ç‡§®", "‡§á‡§Ç‡§ü‡§∞‡•ç‡§®‡§∂‡§ø‡§™", "‡§∏‡•ç‡§µ‡§Ø‡§Ç‡§∏‡•á‡§µ‡§ï", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á","‡§π‡•ã‡§∏‡§≤‡§æ ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§∏‡§æ‡§Æ‡•Ä‡§≤ ‡§µ‡•ç‡§π‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á", "‡§ï‡§∞‡§ø‡§Ö‡§∞ ‡§∏‡§Ç‡§ß‡•Ä", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§ö‡§æ ‡§≠‡§æ‡§ó ‡§¨‡§®‡§æ‡§Ø‡§ö‡§Ç ‡§Ü‡§π‡•á"]
    }

    detected_intern_lang = None
    for lang, keywords in internship_keywords_by_lang.items():
        for kw in keywords:
            match = get_close_matches(user_input_lower, [kw.lower()], n=1, cutoff=0.75)
            if match:
                detected_intern_lang = lang
                break
        if detected_intern_lang:
            break


    if detected_intern_lang:
        internship_responses = {
            "en": (
                "Great! We'd love to have you! üôå\n"
                "Please share your interest and email your Resume to hosla.dalmadal@gmail.com or shantanumproductmanager@gmail.com or shraddhawelfareassociation@gmail.com.\n"
                "üìû You can also call us at +91-7811 009 309 for internship or volunteering opportunities."
            ),
            "hi": (
                "‡§¨‡§π‡•Å‡§§ ‡§¨‡§¢‡§º‡§ø‡§Ø‡§æ! ‡§π‡§Æ‡•á‡§Ç ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã‡§ó‡•Ä ‡§ï‡§ø ‡§Ü‡§™ ‡§π‡§Æ‡§æ‡§∞‡•á ‡§∏‡§æ‡§• ‡§ú‡•Å‡§°‡§º‡•á‡§Ç‡•§ üôå\n"
                "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§∞‡•Å‡§ö‡§ø ‡§™‡§§‡•ç‡§∞ ‡§î‡§∞ ‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á ‡§á‡§∏ ‡§à‡§Æ‡•á‡§≤ ‡§™‡§∞ ‡§≠‡•á‡§ú‡•á‡§Ç: hosla.dalmadal@gmail.com ‡§Ø‡§æ shantanumproductmanager@gmail.com ‡§Ø‡§æ shraddhawelfareassociation@gmail.com\n"
                "üìû ‡§Ü‡§™ ‡§á‡§∏ ‡§®‡§Ç‡§¨‡§∞ ‡§™‡§∞ ‡§ï‡•â‡§≤ ‡§≠‡•Ä ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç: +91-7811 009 309 (‡§∏‡•ã‡§Æ-‡§∂‡•Å‡§ï‡•ç‡§∞, ‡§∏‡•Å‡§¨‡§π 10 ‚Äì ‡§∂‡§æ‡§Æ 6)‡•§"
            ),
            "bn": (
                "‡¶¶‡¶æ‡¶∞‡ßÅ‡¶®! ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶ñ‡ßÅ‡¶∂‡¶ø ‡¶π‡¶¨ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶π‡¶®‡•§ üôå\n"
                "‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßç‡¶∞‡¶π ‡¶è‡¶¨‡¶Ç ‡¶∞‡ßá‡¶ú‡ßÅ‡¶Æ‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡¶® ‡¶è‡¶á ‡¶á‡¶Æ‡ßá‡¶á‡¶≤ ‡¶†‡¶ø‡¶ï‡¶æ‡¶®‡¶æ‡ßü: hosla.dalmadal@gmail.com ‡¶Ö‡¶•‡¶¨‡¶æ shantanumproductmanager@gmail.com ‡¶Ö‡¶•‡¶¨‡¶æ shraddhawelfareassociation@gmail.com\n"
                "üìû ‡¶´‡ßã‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®: +91-7811 009 309 (‡¶∏‡ßã‡¶Æ-‡¶∂‡ßÅ‡¶ï‡ßç‡¶∞, ‡¶∏‡¶ï‡¶æ‡¶≤ ‡ßß‡ß¶‡¶ü‡¶æ ‚Äì ‡¶∏‡¶®‡ßç‡¶ß‡ßç‡¶Ø‡¶æ ‡ß¨‡¶ü‡¶æ)‡•§"
            ),
            "ta": (
                "‡ÆÖ‡Æ∞‡ØÅ‡ÆÆ‡Øà! ‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æü‡Æ©‡Øç ‡Æá‡Æ£‡Øà‡ÆØ ‡Æµ‡Æø‡Æ∞‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æø‡Æ±‡ØÄ‡Æ∞‡Øç‡Æï‡Æ≥‡Øç ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡Øà ‡Æï‡Øá‡Æü‡Øç‡Æü‡ØÅ ‡ÆÆ‡Æï‡Æø‡Æ¥‡Øç‡Æö‡Øç‡Æö‡Æø! üôå\n"
                "‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æµ‡Æø‡Æ∞‡ØÅ‡Æ™‡Øç‡Æ™‡ÆÆ‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ÆØ‡Øã‡Æü‡Øá‡Æü‡Øç‡Æü‡Ææ ‡Æá‡ÆÆ‡ØÜ‡ÆØ‡Æø‡Æ≤‡Øç ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æµ‡ØÅ‡ÆÆ‡Øç: hosla.dalmadal@gmail.com ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ shantanumproductmanager@gmail.com ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ shraddhawelfareassociation@gmail.com\n"
                "üìû ‡ÆÆ‡Øá‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡Æ§‡Æï‡Æµ‡Æ≤‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æá‡Æ®‡Øç‡Æ§ ‡Æé‡Æ£‡Øç‡Æ£‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ¥‡Øà‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç: +91-7811 009 309 (‡Æ§‡Æø‡Æô‡Øç‡Æï‡Æ≥‡Øç‚Äì‡Æµ‡ØÜ‡Æ≥‡Øç‡Æ≥‡Æø, ‡Æï‡Ææ‡Æ≤‡Øà 10 ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ≤‡Øà 6 ‡Æµ‡Æ∞‡Øà)."
            ),
            "te": (
                "‡∞ö‡∞æ‡∞≤‡∞æ ‡∞¨‡∞æ‡∞ó‡±Å‡∞Ç‡∞¶‡∞ø! ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞Æ‡∞æ‡∞ï‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞á‡∞∑‡±ç‡∞ü‡∞Ç! üôå\n"
                "‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡±Ä ‡∞Ü‡∞∏‡∞ï‡±ç‡∞§‡∞ø‡∞®‡∞ø ‡∞™‡∞Ç‡∞ö‡±Å‡∞ï‡±ã‡∞Ç‡∞°‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞Æ‡±Ä ‡∞∞‡±Ü‡∞ú‡±ç‡∞Ø‡±Ç‡∞Æ‡±ç‚Äå‡∞®‡±Å hosla.dalmadal@gmail.com ‡∞≤‡±á‡∞¶‡∞æ shantanumproductmanager@gmail.com ‡∞≤‡±á‡∞¶‡∞æ shraddhawelfareassociation@gmail.com ‡∞ï‡±Å ‡∞á‡∞Æ‡±Ü‡∞Ø‡∞ø‡∞≤‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø.\n"
                "üìû ‡∞á‡∞Ç‡∞ü‡∞∞‡±ç‡∞®‡±ç‚Äå‡∞∑‡∞ø‡∞™‡±ç ‡∞≤‡±á‡∞¶‡∞æ ‡∞µ‡∞æ‡∞≤‡∞Ç‡∞ü‡±Ä‡∞∞‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Ö‡∞µ‡∞ï‡∞æ‡∞∂‡∞æ‡∞≤ ‡∞ï‡±ã‡∞∏‡∞Ç ‡∞Æ‡±Ä‡∞∞‡±Å +91-7811 009 309 ‡∞ï‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞ï‡∞æ‡∞≤‡±ç ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å."
            ),
            "pa": (
                "‡®µ‡®ß‡©Ä‡®Ü! ‡®Ö‡®∏‡©Ä‡®Ç ‡®ñ‡©Å‡®∏‡®º ‡®π‡®æ‡®Ç ‡®ï‡®ø ‡®§‡©Å‡®∏‡©Ä‡®Ç ‡®∏‡®æ‡®°‡©á ‡®®‡®æ‡®≤ ‡®ú‡©Å‡©ú‡®£‡®æ ‡®ö‡®æ‡®π‡©Å‡©∞‡®¶‡©á ‡®π‡©ã! üôå\n"
                "‡®ï‡®ø‡®∞‡®™‡®æ ‡®ï‡®∞‡®ï‡©á ‡®Ü‡®™‡®£‡©Ä ‡®¶‡®ø‡®≤‡®ö‡®∏‡®™‡©Ä ‡®Ö‡®§‡©á ‡®∞‡®ø‡®ú‡®º‡®ø‡®ä‡®Æ ‡®á‡®π‡®®‡®æ‡®Ç ‡®à‡®Æ‡©á‡®≤ ‡®™‡®§‡©á '‡®§‡©á ‡®≠‡©á‡®ú‡©ã: hosla.dalmadal@gmail.com, shantanumproductmanager@gmail.com ‡®ú‡®æ‡®Ç shraddhawelfareassociation@gmail.com\n"
                "üìû ‡®á‡©∞‡®ü‡®∞‡®®‡®∏‡®º‡®ø‡®™ ‡®ú‡®æ‡®Ç ‡®µ‡©ã‡®≤‡©∞‡®ü‡©Ä‡®Ö‡®∞ ‡®Æ‡©å‡®ï‡®ø‡®Ü‡®Ç ‡®≤‡®à +91-7811 009 309 '‡®§‡©á ‡®∏‡©∞‡®™‡®∞‡®ï ‡®ï‡®∞‡©ã‡•§"
            ),
            "mr": (
                "‡§õ‡§æ‡§®! ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§Ü‡§Æ‡§ö‡•ç‡§Ø‡§æ‡§∏‡•ã‡§¨‡§§ ‡§ú‡•ã‡§°‡§≤‡•á ‡§ú‡§æ‡§≤ ‡§Ø‡§æ‡§ö‡§æ ‡§Ü‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ñ‡•Ç‡§™ ‡§Ü‡§®‡§Ç‡§¶ ‡§π‡•ã‡§à‡§≤! üôå\n"
                "‡§ï‡•É‡§™‡§Ø‡§æ ‡§§‡•Å‡§Æ‡§ö‡•Ä ‡§Ü‡§µ‡§° ‡§Ü‡§£‡§ø ‡§∞‡§ø‡§ù‡•ç‡§Ø‡•Å‡§Æ‡•á ‡§™‡•Å‡§¢‡•Ä‡§≤ ‡§à‡§Æ‡•á‡§≤‡§µ‡§∞ ‡§™‡§æ‡§†‡§µ‡§æ: hosla.dalmadal@gmail.com, shantanumproductmanager@gmail.com ‡§ï‡§ø‡§Ç‡§µ‡§æ shraddhawelfareassociation@gmail.com\n"
                "üìû ‡§á‡§Ç‡§ü‡§∞‡•ç‡§®‡§∂‡§ø‡§™ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§∏‡•ç‡§µ‡§Ø‡§Ç‡§∏‡•á‡§µ‡§ï ‡§∏‡§Ç‡§ß‡•Ä‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§ï‡•É‡§™‡§Ø‡§æ +91-7811 009 309 ‡§Ø‡§æ ‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï‡§æ‡§µ‡§∞ ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡§æ."
            )

        }

        reply_text = internship_responses.get(detected_intern_lang, internship_responses["en"])
        return reply_text, False

    # ‚úÖ Direct match for founder query
    founder_phrases = [
        "founder of hosla", "hosla founder", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶§‡¶æ", "‡¶π‡ßã‡¶∏‡¶≤‡¶æ‡¶∞ ‡¶´‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞",
        "‡¶π‡ßã‡¶∏‡¶≤‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶∑‡ßç‡¶†‡¶æ‡¶§‡¶æ", "‡§π‡•ã‡§∏‡§≤‡§æ ‡§ï‡•á ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§™‡§ï", "‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§™‡§ï ‡§π‡•ã‡§∏‡§≤‡§æ"
    ]

    if any(phrase in original_input_lower or phrase in translated_input_lower for phrase in founder_phrases):
        founder_answer = "The Founder of Hosla is Mr. Shantanu Mukhopadhyay"
        try:
            if user_lang != "en":
                founder_answer = translator.translate(founder_answer, dest=user_lang).text
        except Exception as e:
            print("‚ö†Ô∏è Translation error:", e)
        return founder_answer, False

    # ü§ñ Semantic match from FAQ
    user_embedding = model.encode(translated_input, convert_to_tensor=True)
    similarities = util.cos_sim(user_embedding, question_embeddings)[0]
    max_score = float(similarities.max())
    best_idx = int(similarities.argmax())

    if max_score >= 0.45:
        answer = df.iloc[best_idx]["Answer"]
        try:
            if user_lang != "en":
                translated = translator.translate(answer, dest=user_lang)
                if translated.text.strip():
                    answer = translated.text
        except Exception as e:
            print("‚ö†Ô∏è Translation error:", e)
        return answer, False

    # üß† Fuzzy matching fallback for minor typos
    best_match = get_close_matches(translated_input_lower, df["Question"].apply(normalize_text).tolist(), n=1, cutoff=0.6)
    if best_match:
        match_index = df[df["Question"].apply(lambda q: normalize_text(q) == best_match[0])].index[0]
        answer = df.iloc[match_index]["Answer"]
        try:
            if user_lang != "en":
                translated = translator.translate(answer, dest=user_lang)
                if translated.text.strip():
                    answer = translated.text
        except Exception as e:
            print("‚ö†Ô∏è Translation error:", e)
        return answer, False


     # üè• Health-related query
    if is_health_query(translated_input):
        health_info = fetch_medline_info(translated_input, user_lang)
        if health_info:
            return health_info, False

    # ‚ùå Fallback response
    fallback = "ü§ñ I'm not sure about that yet. Please contact Hosla at üìû7811009309 for more information."
    try:
        if user_lang != "en":
            fallback = translator.translate(fallback, dest=user_lang).text
    except:
        pass

    log_unknown_question(user_input, "Guest" if user_is_guest else "Member")
    return fallback, True

last_logged = {}

def log_unknown_question(query, user_name):
    global last_logged
    ist = pytz.timezone('Asia/Kolkata')
    now = datetime.now(ist)  # Current time in IST
    key = (query.strip().lower(), user_name.strip().lower())

    # Cooldown check (10 seconds)
    if key in last_logged and (now - last_logged[key]).total_seconds() < 10:
        print("[HUGGINGFACE LOG] Skipping duplicate log.")
        return

    last_logged[key] = now
    try:
        timestamp = now.strftime("%d-%m-%Y %H:%M:%S")
        data = {
            "question": query,
            "asked_by": user_name,
            "timestamp": timestamp
        }
        webhook_url = "https://script.google.com/macros/s/AKfycbxAohOvjPAnEYCEnaePL3u2FnQiUIGf347PTlP89vX7IW15kh3YOHbi7nCX_jKLlCpg/exec"
        print("[HUGGINGFACE LOG] Sending to Google Sheets:", data)
        res = requests.post(webhook_url, data=data, timeout=5)
        print("[HUGGINGFACE LOG] Google Sheets responded:", res.status_code, res.text)
    except Exception as e:
        print("[HUGGINGFACE LOG] Error logging to Google Sheet:", e)

        
__all__ = ["get_answer_from_faq", "detect_language_safe", "log_unknown_question"]
